{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test1covid19(1712255).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cti812-ZeN4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3375164a-d341-45ba-a8ac-8030384a750f"
      },
      "source": [
        "! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "! wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-3.0.0-preview2-bin-hadoop3.2/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/streaming/AFINN-111.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/iris_libsvm.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/pagerank_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/pic_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/als/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/als/test.data\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/ridge-data/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_movielens_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_svm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/kmeans_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/kittens/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/origin/license.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/images/license.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_lda_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/gmm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/graphx/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/graphx/followers.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/data/graphx/users.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-javassist.html\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-protobuf.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-CC0.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-re2j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-vis.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-mustache.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-arpack.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-javolution.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-join.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-machinist.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-py4j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-slf4j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-heapq.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-scopt.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-scala.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-netlib.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jline.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jquery.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-modernizr.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jodd.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-automaton.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-minlog.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-antlr.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-spire.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-f2j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-janino.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-zstd.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-paranamer.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-datatables.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-kryo.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/licenses/LICENSE-respond.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/jars/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/users.orc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/people.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/employees.json\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/people.json\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/people.csv\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/users.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/kv1.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/partitioned_users.orc/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/partitioned_users.orc/favorite_color=red/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/partitioned_users.orc/favorite_color=red/users.orc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/partitioned_users.orc/favorite_color=__HIVE_DEFAULT_PARTITION__/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/partitioned_users.orc/favorite_color=__HIVE_DEFAULT_PARTITION__/users.orc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/partitioned_users.orc/do_not_read_this.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/full_user.avsc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/user.avsc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/resources/users.avro\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scripts/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/als_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/transitive_closure.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/status_api_demo.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/kmeans.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/pi.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/hive.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/basic.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/arrow.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sql/datasource.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/sort.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/pagerank.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/logistic_regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/python/als.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/fpm.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/glm.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/mlp.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/lda.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/survreg.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/ml.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/gbt.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/als.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/logit.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/kstest.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/data-manipulation.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/dataframe.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/conf/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/conf/slaves.template\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/conf/spark-env.sh.template\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/conf/log4j.properties.template\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/conf/spark-defaults.conf.template\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/conf/fairscheduler.xml.template\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/conf/metrics.properties.template\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-core-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/token-provider-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerby-config-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jpam-1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-shims-0.23-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/okapi-shade-0.4.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-configuration2-2.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/httpclient-4.5.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/avro-1.8.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jsr305-3.0.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/slf4j-api-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-simplekdc-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/JTransforms-3.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-yarn-client-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/orc-core-1.5.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-mllib-local_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jersey-server-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-yarn-server-common-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/scala-library-2.12.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-dataformat-yaml-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/parquet-common-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/zstd-jni-1.4.4-3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-serde-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/ST4-4.0.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/libthrift-0.12.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-hive-thriftserver_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/dnsjava-2.1.7.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-daemon-1.0.13.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-shims-scheduler-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/javassist-3.22.0-CR2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/stax-api-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/json4s-ast_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerby-pkix-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/snappy-java-1.1.7.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-graph_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-common-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/pyrolite-4.30.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/metrics-json-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-exec-2.3.6-core.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/super-csv-2.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-common-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/json4s-scalap_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-databind-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/log4j-1.2.17.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/javolution-5.5.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spire_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-graphx_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/zookeeper-3.4.14.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/velocity-1.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-tags_2.12-3.0.0-preview2-tests.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jakarta.activation-api-1.2.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-core_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerby-util-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/paranamer-2.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-lang-2.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-kubernetes_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/joda-time-2.10.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-module-paranamer-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/breeze_2.12-1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-sql_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/metrics-jvm-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-auth-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-server-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jersey-media-jaxb-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/activation-1.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-hive_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jersey-hk2-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-net-3.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/woodstox-core-5.0.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/re2j-1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-mesos_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/transaction-api-1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-yarn_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/guice-servlet-4.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/metrics-core-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/py4j-0.10.8.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-compress-1.8.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kubernetes-client-4.6.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/nimbus-jose-jwt-4.41.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-sketch_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kubernetes-model-common-4.6.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spire-macros_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-mllib_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-compiler-3.0.15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/univocity-parsers-2.8.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/json4s-jackson_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/arrow-vector-0.15.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-catalyst_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/okhttp-3.12.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jersey-container-servlet-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/curator-framework-2.13.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/htrace-core4-4.1.0-incubating.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/curator-client-2.13.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/aopalliance-1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jta-1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-crypto-1.0.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-yarn-server-web-proxy-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-util-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/guice-4.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spire-platform_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/json-1.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-client-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-common-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/ehcache-3.3.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/accessors-smart-1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/algebra_2.12-2.0.0-M2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/json4s-core_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jersey-common-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/stax2-api-3.1.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/chill-java-0.9.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-jaxrs-base-2.9.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-mapreduce-client-core-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/minlog-1.3.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/shims-0.7.45.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/mssql-jdbc-6.2.1.jre7.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-cli-1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/logging-interceptor-3.12.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/scala-reflect-2.12.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-core-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/HikariCP-2.5.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/libfb303-0.9.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/objenesis-2.5.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/arrow-memory-0.15.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-network-common_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-io-2.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hk2-api-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/breeze-macros_2.12-1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerby-asn1-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-vector-code-gen-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerby-xdr-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-cli-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-crypto-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-collections-3.2.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-service-rpc-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/ivy-2.4.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-mapreduce-client-jobclient-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/netty-all-4.1.42.Final.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spire-util_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jcip-annotations-1.0-1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-metastore-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-tags_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-graph-api_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-client-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-annotations-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/parquet-format-2.4.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/httpcore-4.4.12.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/parquet-column-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-hdfs-client-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kubernetes-model-4.6.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/metrics-jmx-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/machinist_2.12-0.6.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/json-smart-2.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-llap-common-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/okio-1.15.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/derby-10.12.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-repl_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-yarn-common-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-module-scala_2.12-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/snakeyaml-1.24.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jersey-container-servlet-core-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-math3-3.4.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/oro-2.0.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/lz4-java-1.7.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/orc-mapreduce-1.5.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-jaxrs-json-provider-2.9.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-identity-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jackson-annotations-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/chill_2.12-0.9.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jersey-client-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/opencsv-2.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-kvstore_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/scala-compiler-2.12.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/metrics-graphite-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-yarn-api-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/arrow-format-0.15.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/parquet-jackson-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/generex-1.0.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-streaming_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/parquet-encoding-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-pool-1.5.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/avro-ipc-1.8.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jsp-api-2.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/stream-2.9.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/javax.inject-1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/JLargeArrays-1.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/xz-1.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jaxb-api-2.2.11.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/orc-shims-1.5.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-mapreduce-client-common-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/core-1.1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-codec-1.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-logging-1.1.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/aircompressor-0.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jline-2.14.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-shims-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-lang3-3.9.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-network-shuffle_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/xbean-asm7-shaded-4.15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/okhttp-2.7.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jdo-api-3.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-cypher_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-jdbc-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-dbcp-1.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/gson-2.2.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hadoop-yarn-registry-3.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/janino-3.0.15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-shims-common-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jodd-core-3.5.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/antlr4-runtime-4.7.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/kerb-admin-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-beeline-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/automaton-1.11-8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/spark-launcher_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/guava-14.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-text-1.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/hive-storage-api-2.6.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/jars/commons-httpclient-3.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/NOTICE\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/.gitignore\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/heapq3.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tuning.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/clustering.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/wrapper.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/tree.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/fpm.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/linalg/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/base.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/common.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/evaluation.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/classification.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/param/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/param/shared.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/image.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/pipeline.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/recommendation.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/stat.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/ml/regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/shuffle.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/context.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/kinesis.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/dstream.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/streaming/listener.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/rddsampler.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/context.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/rdd.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_serializers.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_profiler.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_util.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_join.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_rdd.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_context.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_daemon.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_worker.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/_globals.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/resultiterable.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/resourceinformation.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/find_spark_home.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/version.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/profiler.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/cloudpickle.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/statcounter.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/status.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/context.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/functions.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_cogrouped_map.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_iter.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/group.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/types.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/streaming.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/window.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/avro/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/avro/functions.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/column.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/udf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/cogroup.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/clustering.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/tree.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/fpm.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/linalg/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/common.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/random.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/evaluation.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/classification.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/recommendation.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/stat/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/stat/test.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/mllib/regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/taskcontext.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/serializers.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/traceback_utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/storagelevel.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/worker.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/testing/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/testing/sqlutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/testing/streamingutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/testing/utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/testing/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/testing/mlutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/testing/mllibutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/java_gateway.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/broadcast.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/join.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/files.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/accumulators.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/daemon.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/shell.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/lib/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/lib/pyspark.zip\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/hello/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/hello/hello.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/hello/sub_hello/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/userlib-0.1.zip\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/people.json\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/ages.csv\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/people1.json\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/people_array.json\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/text-test.txt\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/userlibrary.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_coverage/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_coverage/conf/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_coverage/sitecustomize.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/test_coverage/coverage_daemon.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/run-tests\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/setup.cfg\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/run-tests.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/pylintrc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/run-tests-with-coverage\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/README.md\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/.coveragerc\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/_static/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/_static/copybutton.js\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/_static/pyspark.js\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/_static/pyspark.css\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/pyspark.ml.rst\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/_templates/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/_templates/layout.html\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/pyspark.sql.rst\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/make.bat\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/Makefile\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/make2.bat\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/index.rst\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/pyspark.mllib.rst\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/pyspark.streaming.rst\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/docs/pyspark.rst\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/MANIFEST.in\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/python/setup.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-submit.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/pyspark.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-sql\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/beeline\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/pyspark\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/load-spark-env.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/load-spark-env.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-submit\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/run-example.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/find-spark-home.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-shell2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/docker-image-tool.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/sparkR2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/sparkR\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/find-spark-home\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-shell.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-submit2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-class2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-sql2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-class\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-sql.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/pyspark2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/beeline.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/run-example\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-shell\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/spark-class.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/bin/sparkR.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-slave.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-history-server.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-slaves.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-history-server.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-thriftserver.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-master.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-master.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-slave.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-slaves.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-all.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/slaves.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/spark-daemons.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/start-all.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/spark-daemon.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/stop-thriftserver.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/sbin/spark-config.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/RELEASE\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/README.md\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/yarn/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/yarn/spark-3.0.0-preview2-yarn-shuffle.jar\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/LICENSE\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/INDEX\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/help/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/help/paths.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/help/AnIndex\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/worker/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/worker/worker.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/html/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/html/00Index.html\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/html/R.css\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/Meta/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/NAMESPACE\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/profile/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/profile/general.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/profile/shell.R\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/R/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/R/SparkR\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/R/lib/sparkr.zip\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/tests/py_container_checks.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/tests/pyfiles.py\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.0.0-preview2-bin-hadoop3.2/kubernetes/dockerfiles/spark/Dockerfile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lgq4SQwfRs4"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw49vtaTfWWE"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import desc\n",
        "from pyspark.sql.functions import asc\n",
        "from pyspark.sql.functions import sum as Fsum\n",
        "\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Y8UPPDN0o5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma94jU2vfbzz"
      },
      "source": [
        "spark = SparkSession \\\n",
        "        .builder\\\n",
        "        .appName(\"Did age affect to increasing death in COVID19\")\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS6yyX_7fqN0"
      },
      "source": [
        "path = \"data/COVID19.csv\"\n",
        "age_death = spark.read.csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQJYT3tJgMXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5c773b10-39d0-480d-ad00-d93d70ec1bcf"
      },
      "source": [
        "age_death.take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_c0='ID', _c1='age', _c2='sex', _c3='city', _c4='province', _c5='country', _c6='latitude', _c7='longitude', _c8='geo_resolution', _c9='date_onset_symptoms', _c10='date_admission_hospital', _c11='date_confirmation', _c12='symptoms', _c13='lives_in_Wuhan', _c14='travel_history_dates', _c15='travel_history_location', _c16='reported_market_exposure', _c17='additional_information', _c18='chronic_disease_binary', _c19='chronic_disease', _c20='source', _c21='sequence_available', _c22='outcome', _c23='date_death_or_discharge', _c24='notes_for_discussion', _c25='location', _c26='admin3', _c27='admin2', _c28='admin1', _c29='country_new', _c30='admin_id', _c31='data_moderator_initials', _c32='travel_history_binary'),\n",
              " Row(_c0='000-1-1', _c1=None, _c2='male', _c3='Shek Lei', _c4='Hong Kong', _c5='China', _c6='22.3650193', _c7='114.133808', _c8='point', _c9=None, _c10=None, _c11='14.02.2020', _c12=None, _c13=None, _c14='22.01.2020', _c15='China', _c16=None, _c17='Case 55; mainland China travel via the Lok Ma Chau border crossing', _c18='False', _c19=None, _c20='https://www.scmp.com/news/hong-kong/health-environment/article/3050681/coronavirus-hong-kong-confirms-three-news-cases', _c21=None, _c22='critical condition, intubated as of 14.02.2020', _c23=None, _c24=None, _c25='Shek Lei', _c26=None, _c27=None, _c28='Hong Kong', _c29='China', _c30='8051.0', _c31=None, _c32=None),\n",
              " Row(_c0='000-1-10', _c1='78', _c2='male', _c3='Vo Euganeo', _c4='Veneto', _c5='Italy', _c6='45.2977477', _c7='11.6583815', _c8='point', _c9=None, _c10=None, _c11='21.02.2020', _c12=None, _c13=None, _c14=None, _c15=None, _c16=None, _c17='Hospitalized on 12.02.2020 for other reasons', _c18='False', _c19=None, _c20='https://www.corriere.it/cronache/20_febbraio_21/coronavirus-italia-cosa-sappiamo-casi-codogno-lombardia-27fa736c-548b-11ea-9196-da7d305401b7.shtml', _c21=None, _c22='death', _c23='22.02.2020', _c24=None, _c25=\"Vo' Euganeo\", _c26=None, _c27=None, _c28='Veneto', _c29='Italy', _c30='8978.0', _c31=None, _c32=None),\n",
              " Row(_c0='000-1-100', _c1='61', _c2='female', _c3=None, _c4=None, _c5='Singapore', _c6='1.35346', _c7='103.8151', _c8='admin0', _c9=None, _c10=None, _c11='14.02.2020', _c12=None, _c13=None, _c14=None, _c15=None, _c16=None, _c17='Case 65; family member of Case 50, a DBS employee, as is Case 55', _c18='False', _c19=None, _c20='https://www.channelnewsasia.com/news/singapore/coronavirus-covid-19-singapore-cases-grace-assembly-of-god--12434646', _c21=None, _c22=None, _c23=None, _c24=None, _c25=None, _c26=None, _c27=None, _c28=None, _c29=None, _c30=None, _c31=None, _c32=None),\n",
              " Row(_c0='https://bnonews.com/wp-content/uploads/2020/02/2172020Cases.pdf\"', _c1=None, _c2='discharge', _c3='17.02.2020', _c4=None, _c5=None, _c6=None, _c7=None, _c8=None, _c9='Singapore', _c10='201.0', _c11=None, _c12=None, _c13=None, _c14=None, _c15=None, _c16=None, _c17=None, _c18=None, _c19=None, _c20=None, _c21=None, _c22=None, _c23=None, _c24=None, _c25=None, _c26=None, _c27=None, _c28=None, _c29=None, _c30=None, _c31=None, _c32=None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWdXudmPgaqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e01c485e-3963-4884-f981-edce4525bab9"
      },
      "source": [
        "age_death.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+---------+----------+---------+---------+----------+----------+--------------+-------------------+--------------------+-----------------+--------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+------+------+---------+-----------+--------+--------------------+--------------------+\n",
            "|                 _c0| _c1|      _c2|       _c3|      _c4|      _c5|       _c6|       _c7|           _c8|                _c9|                _c10|             _c11|    _c12|          _c13|                _c14|                _c15|                _c16|                _c17|                _c18|           _c19|                _c20|              _c21|                _c22|                _c23|                _c24|       _c25|  _c26|  _c27|     _c28|       _c29|    _c30|                _c31|                _c32|\n",
            "+--------------------+----+---------+----------+---------+---------+----------+----------+--------------+-------------------+--------------------+-----------------+--------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+------+------+---------+-----------+--------+--------------------+--------------------+\n",
            "|                  ID| age|      sex|      city| province|  country|  latitude| longitude|geo_resolution|date_onset_symptoms|date_admission_ho...|date_confirmation|symptoms|lives_in_Wuhan|travel_history_dates|travel_history_lo...|reported_market_e...|additional_inform...|chronic_disease_b...|chronic_disease|              source|sequence_available|             outcome|date_death_or_dis...|notes_for_discussion|   location|admin3|admin2|   admin1|country_new|admin_id|data_moderator_in...|travel_history_bi...|\n",
            "|             000-1-1|null|     male|  Shek Lei|Hong Kong|    China|22.3650193|114.133808|         point|               null|                null|       14.02.2020|    null|          null|          22.01.2020|               China|                null|Case 55; mainland...|               False|           null|https://www.scmp....|              null|critical conditio...|                null|                null|   Shek Lei|  null|  null|Hong Kong|      China|  8051.0|                null|                null|\n",
            "|            000-1-10|  78|     male|Vo Euganeo|   Veneto|    Italy|45.2977477|11.6583815|         point|               null|                null|       21.02.2020|    null|          null|                null|                null|                null|Hospitalized on 1...|               False|           null|https://www.corri...|              null|               death|          22.02.2020|                null|Vo' Euganeo|  null|  null|   Veneto|      Italy|  8978.0|                null|                null|\n",
            "|           000-1-100|  61|   female|      null|     null|Singapore|   1.35346|  103.8151|        admin0|               null|                null|       14.02.2020|    null|          null|                null|                null|                null|Case 65; family m...|               False|           null|https://www.chann...|              null|                null|                null|                null|       null|  null|  null|     null|       null|    null|                null|                null|\n",
            "|https://bnonews.c...|null|discharge|17.02.2020|     null|     null|      null|      null|          null|          Singapore|               201.0|             null|    null|          null|                null|                null|                null|                null|                null|           null|                null|              null|                null|                null|                null|       null|  null|  null|     null|       null|    null|                null|                null|\n",
            "+--------------------+----+---------+----------+---------+---------+----------+----------+--------------+-------------------+--------------------+-----------------+--------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+------+------+---------+-----------+--------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Foz8YQhTa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "414f4ef8-4ca4-47bb-dda0-7ab3a2feebdf"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT age,date_death_or_discharge as death\n",
        "          FROM age_death\n",
        "          '''\n",
        "          ).collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: age_death; line 3 pos 15;\n'Project ['age, 'date_death_or_discharge AS death#378]\n+- 'UnresolvedRelation [age_death]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:153)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d17ed218412f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mSELECT\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_death_or_discharge\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           '''\n\u001b[0m\u001b[1;32m      5\u001b[0m           ).collect()\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: age_death; line 3 pos 15;\n'Project ['age, 'date_death_or_discharge AS death#378]\n+- 'UnresolvedRelation [age_death]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oexCdn_zhojB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "9f4e5281-1b13-43b1-ecfc-068deb8ab7cc"
      },
      "source": [
        "country_log.createOrReplaceTempView(\"age_death_table\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ee2fa590cb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcountry_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age_death_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'country_log' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWUrvG44h4W8"
      },
      "source": [
        "age_death.createOrReplaceTempView(\"age_death_table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RreCiPk8iBjn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b7adb8a4-7ede-4ecb-d5e2-dc5652811e22"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT *\n",
        "          FROM age_death_table\n",
        "          LIMIT 6\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+---------+--------------+---------+---------+----------+------------------+--------------+-------------------+--------------------+-----------------+--------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+------+--------------+---------+-----------+--------+--------------------+--------------------+\n",
            "|                 _c0| _c1|      _c2|           _c3|      _c4|      _c5|       _c6|               _c7|           _c8|                _c9|                _c10|             _c11|    _c12|          _c13|                _c14|                _c15|                _c16|                _c17|                _c18|           _c19|                _c20|              _c21|                _c22|                _c23|                _c24|       _c25|  _c26|          _c27|     _c28|       _c29|    _c30|                _c31|                _c32|\n",
            "+--------------------+----+---------+--------------+---------+---------+----------+------------------+--------------+-------------------+--------------------+-----------------+--------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+------+--------------+---------+-----------+--------+--------------------+--------------------+\n",
            "|                  ID| age|      sex|          city| province|  country|  latitude|         longitude|geo_resolution|date_onset_symptoms|date_admission_ho...|date_confirmation|symptoms|lives_in_Wuhan|travel_history_dates|travel_history_lo...|reported_market_e...|additional_inform...|chronic_disease_b...|chronic_disease|              source|sequence_available|             outcome|date_death_or_dis...|notes_for_discussion|   location|admin3|        admin2|   admin1|country_new|admin_id|data_moderator_in...|travel_history_bi...|\n",
            "|             000-1-1|null|     male|      Shek Lei|Hong Kong|    China|22.3650193|        114.133808|         point|               null|                null|       14.02.2020|    null|          null|          22.01.2020|               China|                null|Case 55; mainland...|               False|           null|https://www.scmp....|              null|critical conditio...|                null|                null|   Shek Lei|  null|          null|Hong Kong|      China|  8051.0|                null|                null|\n",
            "|            000-1-10|  78|     male|    Vo Euganeo|   Veneto|    Italy|45.2977477|        11.6583815|         point|               null|                null|       21.02.2020|    null|          null|                null|                null|                null|Hospitalized on 1...|               False|           null|https://www.corri...|              null|               death|          22.02.2020|                null|Vo' Euganeo|  null|          null|   Veneto|      Italy|  8978.0|                null|                null|\n",
            "|           000-1-100|  61|   female|          null|     null|Singapore|   1.35346|          103.8151|        admin0|               null|                null|       14.02.2020|    null|          null|                null|                null|                null|Case 65; family m...|               False|           null|https://www.chann...|              null|                null|                null|                null|       null|  null|          null|     null|       null|    null|                null|                null|\n",
            "|https://bnonews.c...|null|discharge|    17.02.2020|     null|     null|      null|              null|          null|          Singapore|               201.0|             null|    null|          null|                null|                null|                null|                null|                null|           null|                null|              null|                null|                null|                null|       null|  null|          null|     null|       null|    null|                null|                null|\n",
            "|          000-1-1000|null|     null|Zhengzhou City|    Henan|    China|  34.62931|113.46799999999999|        admin2|               null|                null|       26.01.2020|    null|          null|                null|                null|                null|                null|               False|           null|https://news.163....|              null|                null|                null|                null|       null|  null|Zhengzhou City|    Henan|      China| 10115.0|                null|                null|\n",
            "+--------------------+----+---------+--------------+---------+---------+----------+------------------+--------------+-------------------+--------------------+-----------------+--------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+------+--------------+---------+-----------+--------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8vpEExhiT6V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95327bfc-d4f4-41f0-f454-fcaebcb7e804"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT age,date_death_or_discharge as death\n",
        "          FROM age_death_table\n",
        "          LIMIT 10\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`age`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Project ['age, 'date_death_or_discharge AS death#644]\n      +- SubqueryAlias `age_death_table`\n         +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:153)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:153)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3ab55aa24345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mLIMIT\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           '''\n\u001b[0m\u001b[1;32m      6\u001b[0m           ).show()\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`age`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Project ['age, 'date_death_or_discharge AS death#644]\n      +- SubqueryAlias `age_death_table`\n         +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiakFMXZinDb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "aee171d5-cbd8-46ed-c40e-ff56bd5a8ff7"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          LIMIT 10\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------------------+\n",
            "| _c1|                _c23|\n",
            "+----+--------------------+\n",
            "| age|date_death_or_dis...|\n",
            "|null|                null|\n",
            "|  78|          22.02.2020|\n",
            "|  61|                null|\n",
            "|null|                null|\n",
            "|null|                null|\n",
            "|null|                null|\n",
            "|null|                null|\n",
            "|null|                null|\n",
            "|null|                null|\n",
            "+----+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-9C1qxmi8s7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cad0efaf-74f9-45c7-dec6-ff52f5c01559"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 desc\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ParseException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input 'desc' expecting <EOF>(line 4, pos 20)\n\n== SQL ==\n\n          SELECT _c1,_c23\n          FROM age_death_table\n          WHERE _c1 desc\n--------------------^^^\n          \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:263)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:130)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:76)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:605)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7aa15683fc65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           '''\n\u001b[0m\u001b[1;32m      6\u001b[0m           ).show()\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParseException\u001b[0m: \nextraneous input 'desc' expecting <EOF>(line 4, pos 20)\n\n== SQL ==\n\n          SELECT _c1,_c23\n          FROM age_death_table\n          WHERE _c1 desc\n--------------------^^^\n          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pIzO-brjR6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "f3b3d99d-9ed7-4e9b-f1c1-db9a4bbd4f80"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          ORDER BY _c1 DESC\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "|_c1|                _c23|\n",
            "+---+--------------------+\n",
            "|age|date_death_or_dis...|\n",
            "| 99|          03.05.2020|\n",
            "| 99|                null|\n",
            "| 98|                null|\n",
            "| 97|                null|\n",
            "| 96|                null|\n",
            "| 96|                null|\n",
            "| 96|                null|\n",
            "| 96|                null|\n",
            "| 95|                null|\n",
            "| 95|          03.03.2020|\n",
            "| 95|          03.03.2020|\n",
            "| 95|                null|\n",
            "| 95|                null|\n",
            "| 94|                null|\n",
            "| 94|          14.02.2020|\n",
            "| 94|                null|\n",
            "| 94|                null|\n",
            "| 94|                null|\n",
            "| 93|                null|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXQSWq4ojjJg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "630d2350-094c-4e4e-dbbc-6f786252fdf8"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          ORDER BY ASC\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`ASC`' given input columns: [age_death_table._c1, age_death_table._c23]; line 4 pos 19;\n'Sort ['ASC ASC NULLS FIRST], true\n+- Project [_c1#15, _c23#37]\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-03079bb0eb12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mORDER\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mASC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           '''\n\u001b[0m\u001b[1;32m      6\u001b[0m           ).show()\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`ASC`' given input columns: [age_death_table._c1, age_death_table._c23]; line 4 pos 19;\n'Sort ['ASC ASC NULLS FIRST], true\n+- Project [_c1#15, _c23#37]\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVhqhfUbjt7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a8498c0-8706-48ec-a802-8e5203cf8d89"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          ORDER BY _c1 ASC\n",
        "          '''\n",
        "          ).show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.show of DataFrame[_c1: string, _c23: string]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSaE4xv6j-Tw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "ba87dc51-0b86-46b9-8c50-a4a11c04d0ed"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          ORDER BY _c1 ASC\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+\n",
            "| _c1|_c23|\n",
            "+----+----+\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L29QlO0-kEMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6c380774-81ae-42cb-fcae-3f5fb89cf321"
      },
      "source": [
        "DELETE FROM table WHERE edit_user IS NULL\n",
        "spark.sql('''\n",
        "          DELETE FROM age_death WHERE edit_user IS NULL\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-be746423061f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    DELETE FROM table WHERE edit_user IS NULL\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLKFCSYPki4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6f0ef15-568b-4a00-fd4d-995bdbdd30f5"
      },
      "source": [
        "spark.sql('''\n",
        "          DELETE FROM age_death WHERE edit_user IS NULL\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: age_death; line 2 pos 10;\n'DeleteFromTable isnull('edit_user)\n+- 'UnresolvedRelation [age_death]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:153)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:153)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e5ffa800780e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m spark.sql('''\n\u001b[1;32m      2\u001b[0m           \u001b[0mDELETE\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0mage_death\u001b[0m \u001b[0mWHERE\u001b[0m \u001b[0medit_user\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNULL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           '''\n\u001b[0m\u001b[1;32m      4\u001b[0m           ).show()\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: age_death; line 2 pos 10;\n'DeleteFromTable isnull('edit_user)\n+- 'UnresolvedRelation [age_death]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C4QCXGqkmwx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "b080af99-297a-4c7c-a457-6e7d64b7ab97"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          ORDER BY _c1 ASC\n",
        "          LIMIT 50\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+\n",
            "| _c1|_c23|\n",
            "+----+----+\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "|null|null|\n",
            "+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LHVbPwxy2SN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC5fJKCKk4az",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "381299a3-2d35-4083-95b1-4eb856571f62"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 > 50\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "|_c1|      _c23|\n",
            "+---+----------+\n",
            "| 78|22.02.2020|\n",
            "| 61|      null|\n",
            "| 66|      null|\n",
            "| 51|      null|\n",
            "| 68|      null|\n",
            "| 53|      null|\n",
            "| 58|      null|\n",
            "| 88|      null|\n",
            "| 78|      null|\n",
            "| 52|      null|\n",
            "| 60|      null|\n",
            "| 56|02.03.2020|\n",
            "| 79|29.02.2020|\n",
            "| 60|      null|\n",
            "| 82|      null|\n",
            "| 69|      null|\n",
            "| 55|      null|\n",
            "| 62|      null|\n",
            "| 71|29.02.2020|\n",
            "| 64|      null|\n",
            "+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlLXa8j0lBaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "ca9a137e-5448-4e2b-aa25-0087ef75a7c2"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c23\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 < 50\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "|_c1|      _c23|\n",
            "+---+----------+\n",
            "| 27|      null|\n",
            "| 17|      null|\n",
            "| 26|      null|\n",
            "| 30|      null|\n",
            "| 35|      null|\n",
            "| 27|      null|\n",
            "| 28|      null|\n",
            "| 33|      null|\n",
            "| 33|      null|\n",
            "| 33|      null|\n",
            "| 45|      null|\n",
            "| 24|      null|\n",
            "| 29|      null|\n",
            "| 21|      null|\n",
            "| 28|20.02.2020|\n",
            "| 26|02.03.2020|\n",
            "| 25|      null|\n",
            "| 40|      null|\n",
            "| 43|26.02.2020|\n",
            "| 33|      null|\n",
            "+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYarh2zulRHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "6524f31e-4a6b-4ba4-d54d-17ef4f5c4c83"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c22\n",
        "          FROM age_death_table\n",
        "          WHERE _c22 = \"death\"\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|  _c1| _c22|\n",
            "+-----+-----+\n",
            "|   78|death|\n",
            "|   68|death|\n",
            "|   88|death|\n",
            "|   95|death|\n",
            "|80-89|death|\n",
            "|   60|death|\n",
            "|   79|death|\n",
            "|   52|death|\n",
            "|   66|death|\n",
            "| null|death|\n",
            "|   82|death|\n",
            "|   73|death|\n",
            "|80-89|death|\n",
            "|60-69|death|\n",
            "| null|death|\n",
            "| null|death|\n",
            "|   77|death|\n",
            "|80-89|death|\n",
            "|60-69|death|\n",
            "| null|death|\n",
            "+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtl19GGblwgx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "23c61086-b4a0-4f7a-98a3-eec2f1440eaa"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c22\n",
        "          FROM age_death_table\n",
        "          WHERE _c22 = \"Recovered\"\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------+\n",
            "| _c1|     _c22|\n",
            "+----+---------+\n",
            "|  20|Recovered|\n",
            "|  40|Recovered|\n",
            "|null|Recovered|\n",
            "|null|Recovered|\n",
            "|  78|Recovered|\n",
            "|  54|Recovered|\n",
            "|  33|Recovered|\n",
            "|  46|Recovered|\n",
            "|  42|Recovered|\n",
            "|null|Recovered|\n",
            "|null|Recovered|\n",
            "|  71|Recovered|\n",
            "|  28|Recovered|\n",
            "|  52|Recovered|\n",
            "|null|Recovered|\n",
            "|null|Recovered|\n",
            "|null|Recovered|\n",
            "|null|Recovered|\n",
            "|null|Recovered|\n",
            "|null|Recovered|\n",
            "+----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ7jP4-gmRa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8039a2b2-6f2f-47a3-98cc-7641de57f764"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c22\n",
        "          FROM age_death_table\n",
        "          WHERE _c22 = \"Critical condition\"\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------------+\n",
            "| _c1|              _c22|\n",
            "+----+------------------+\n",
            "|null|Critical condition|\n",
            "+----+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0zB4xQQneLF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "2ff01b79-6e64-4c64-e993-fa9acb2a551f"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c22\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 > 70\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "|_c1|     _c22|\n",
            "+---+---------+\n",
            "| 78|    death|\n",
            "| 88|     null|\n",
            "| 78|     null|\n",
            "| 79|discharge|\n",
            "| 82|     null|\n",
            "| 71|discharge|\n",
            "| 71|     null|\n",
            "| 82|     null|\n",
            "| 73|     null|\n",
            "| 78|     null|\n",
            "| 96|     null|\n",
            "| 75|     null|\n",
            "| 88|    death|\n",
            "| 80|     null|\n",
            "| 77|     null|\n",
            "| 72|     null|\n",
            "| 95|    death|\n",
            "| 81|     null|\n",
            "| 75|     null|\n",
            "| 85|     null|\n",
            "+---+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUJ2uaBrthpO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gOl6EXVnnHG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "d3e9a37a-5c8f-43db-85e0-e0aa26384057"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c22\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 > 70\n",
        "          ORDER BY _c1 ASC\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "|_c1|     _c22|\n",
            "+---+---------+\n",
            "|101|     null|\n",
            "|105|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|Recovered|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "| 71|     null|\n",
            "+---+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF4IKm2Ztj8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "364ec01c-eb0d-4225-c1e5-6841b62a3783"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c22\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+\n",
            "|_c1|   _c22|\n",
            "+---+-------+\n",
            "|age|outcome|\n",
            "| 78|  death|\n",
            "| 61|   null|\n",
            "| 66|   null|\n",
            "| 27|   null|\n",
            "| 17|   null|\n",
            "| 51|   null|\n",
            "| 68|   null|\n",
            "| 26|   null|\n",
            "| 30|   null|\n",
            "| 53|   null|\n",
            "| 35|   null|\n",
            "| 27|   null|\n",
            "| 28|   null|\n",
            "| 58|   null|\n",
            "| 33|   null|\n",
            "| 33|   null|\n",
            "| 50|   null|\n",
            "| 33|   null|\n",
            "| 45|   null|\n",
            "+---+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNzgv32Itxy9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "d3f54b59-03bc-471f-eba2-968fb395680b"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1,_c22\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL AND _c22 IS NOT NULL\n",
        "          ORDER BY _c1 ASC\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------------+\n",
            "| _c1|            _c22|\n",
            "+----+----------------+\n",
            "|   0|            died|\n",
            "|   0|       recovered|\n",
            "|   0|            died|\n",
            "|   0|       recovered|\n",
            "|   0|            died|\n",
            "|0.25|       discharge|\n",
            "|0.25|       discharge|\n",
            "| 0.5|      discharged|\n",
            "|0.75|stable condition|\n",
            "|   1|       discharge|\n",
            "|   1|       Recovered|\n",
            "|   1|       discharge|\n",
            "|   1|            died|\n",
            "|   1|            died|\n",
            "|   1|      discharged|\n",
            "|   1|       recovered|\n",
            "|   1|    Hospitalized|\n",
            "|   1|            died|\n",
            "|   1|    Hospitalized|\n",
            "|  10|          Stable|\n",
            "+----+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJKYXcOvuPk1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fadef820-a7f4-47fa-e4ba-51ab88a29a81"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT mean(_c1) as mean_age\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|          mean_age|\n",
            "+------------------+\n",
            "|40.961640489222745|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_Oddxehu3Pv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa75c554-66d3-4990-8399-56d63f807ce1"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT mean(_c1) as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c22 = \"death\"\n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#900, _c22#36 AS outcome#901]\n+- Filter (isnotnull(_c1#15) AND (_c22#36 = death))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10$adapted(CheckAnalysis.scala:255)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:282)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b855821beef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_c22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"death\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           '''\n\u001b[0m\u001b[1;32m      6\u001b[0m           ).show()\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#900, _c22#36 AS outcome#901]\n+- Filter (isnotnull(_c1#15) AND (_c22#36 = death))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_eQHmzFvlCm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "17b3c05e-3d1d-4898-cc2a-32389c36ae82"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL \n",
        "          Join(\n",
        "            '''\n",
        "            SELECT _c22\n",
        "            FROM age_death_table\n",
        "            WHERE _c2 is not null and\n",
        "            _c2 = \"death\"\n",
        "            ''')\n",
        "          ) \n",
        "          \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-09eb86662b55>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    SELECT _c22\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKpJb-CRw9EV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1f6bfa2-2607-4c74-f529-ecb55c4c36bb"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c1 = 40\n",
        "          ''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#903, _c22#36 AS outcome#904]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-49b166dd367c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''')\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#903, _c22#36 AS outcome#904]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at2bIiDbxUvf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b7cf7893-c78e-4b23-8e81-87273765bba3"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 = 40 AND _c22 = \"death\"\n",
        "          ''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#905, _c22#36 AS outcome#906]\n+- Filter ((cast(_c1#15 as int) = 40) AND (_c22#36 = death))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-de059c4c862c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m \u001b[0mAND\u001b[0m \u001b[0m_c22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"death\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''')\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#905, _c22#36 AS outcome#906]\n+- Filter ((cast(_c1#15 as int) = 40) AND (_c22#36 = death))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dskS0ABxc9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f6f0a13-0936-4bb5-debf-5c0378f64b52"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c1 == 40\n",
        "          ''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#907, _c22#36 AS outcome#908]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-4453f23d39dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''')\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#907, _c22#36 AS outcome#908]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhFdjWFeB_LU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXT9bcjTxhV4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "219f20d9-dfa3-472e-bf96-f3eee32a0ed8"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE  _c1 = 40\n",
        "          ''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#909, _c22#36 AS outcome#910]\n+- Filter (cast(_c1#15 as int) = 40)\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d843f34d9324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m  \u001b[0m_c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''')\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#909, _c22#36 AS outcome#910]\n+- Filter (cast(_c1#15 as int) = 40)\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3rRBhexwBp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64188e2c-3cbd-435a-fda6-7d8c5b94ec76"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c1 = 40\n",
        "          ''')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[mean_age: string, outcome: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwMQ5ecHxzo5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "598fb91a-a736-4c14-d591-091167d3168b"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c1 = 40\n",
        "          ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#915, _c22#36 AS outcome#916]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:140)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5e5c07062914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''').show()\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`c1`' given input columns: [age_death_table._c0, age_death_table._c1, age_death_table._c10, age_death_table._c11, age_death_table._c12, age_death_table._c13, age_death_table._c14, age_death_table._c15, age_death_table._c16, age_death_table._c17, age_death_table._c18, age_death_table._c19, age_death_table._c2, age_death_table._c20, age_death_table._c21, age_death_table._c22, age_death_table._c23, age_death_table._c24, age_death_table._c25, age_death_table._c26, age_death_table._c27, age_death_table._c28, age_death_table._c29, age_death_table._c3, age_death_table._c30, age_death_table._c31, age_death_table._c32, age_death_table._c4, age_death_table._c5, age_death_table._c6, age_death_table._c7, age_death_table._c8, age_death_table._c9]; line 2 pos 17;\n'Project ['c1 AS mean_age#915, _c22#36 AS outcome#916]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcnRt5onx3BB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "e05d291d-4e39-4ffa-fe70-48471736cd4b"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c1 = 40\n",
        "          ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+\n",
            "|mean_age|   outcome|\n",
            "+--------+----------+\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|discharged|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "|      40|      null|\n",
            "+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUegWNZvx9Qe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "2d3c31c2-ebfc-4822-95a4-75340b3b23e5"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c22 IS NOT NULL and _c1 = 40\n",
        "          ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------------+\n",
            "|mean_age|         outcome|\n",
            "+--------+----------------+\n",
            "|      40|      discharged|\n",
            "|      40|       Recovered|\n",
            "|      40|           Alive|\n",
            "|      40|           Alive|\n",
            "|      40|           Alive|\n",
            "|      40|          Stable|\n",
            "|      40|stable condition|\n",
            "|      40|       recovered|\n",
            "|      40|       recovered|\n",
            "|      40|       recovered|\n",
            "|      40|            died|\n",
            "|      40|       recovered|\n",
            "|      40|       recovered|\n",
            "|      40|       recovered|\n",
            "|      40|       recovered|\n",
            "|      40|       recovered|\n",
            "|      40|       Recovered|\n",
            "|      40|    Hospitalized|\n",
            "|      40|    Hospitalized|\n",
            "|      40|       Recovered|\n",
            "+--------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ9i1EDyWkcU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quJvErj1y4E4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "4a3a2b05-f081-4f0b-9553-15fdac96c05f"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as age,_c23 as date\n",
        "          FROM age_death_table\n",
        "          WHERE _c23 is not null\n",
        "          ORDER BY _c1 ASC\n",
        "          \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------+\n",
            "| age|      date|\n",
            "+----+----------+\n",
            "|null|23.01.2020|\n",
            "|null|16.02.2020|\n",
            "|null|18.03.2020|\n",
            "|null|16.02.2020|\n",
            "|null|18.02.2020|\n",
            "|null|22.02.2020|\n",
            "|null|28.02.2020|\n",
            "|null|18.02.2020|\n",
            "|null|13.02.2020|\n",
            "|null|13.02.2020|\n",
            "|null|02.02.2020|\n",
            "|null|19.02.2020|\n",
            "|null|24.01.2020|\n",
            "|null|19.02.2020|\n",
            "|null|07.02.2020|\n",
            "|null|02.02.2020|\n",
            "|null|23.02.2020|\n",
            "|null|24.01.2020|\n",
            "|null|06.03.2020|\n",
            "|null|23.01.2020|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbfyNzU4zIFM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "e2e33109-e7c8-4465-cf09-e40fc1f15343"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as age,_c23 as date\n",
        "          FROM age_death_table\n",
        "          WHERE _c23 is not null AND _c1 is not null\n",
        "          ORDER BY _c1 ASC\n",
        "          \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------+\n",
            "| age|      date|\n",
            "+----+----------+\n",
            "|   0|16.05.2020|\n",
            "|   0|12.05.2020|\n",
            "|   0|09.05.2020|\n",
            "|0.25|20.02.2020|\n",
            "|0.25|20.02.2020|\n",
            "| 0.5|22.02.2020|\n",
            "|   1|18.02.2020|\n",
            "|   1|18.02.2020|\n",
            "|   1|21.05.2020|\n",
            "|   1|17.02.2020|\n",
            "|   1|12.05.2020|\n",
            "|   1|08.05.2020|\n",
            "|  10|14.05.2020|\n",
            "|  12|07.03.2020|\n",
            "|  12|07.03.2020|\n",
            "|  13|16.03.2020|\n",
            "|  13|10.03.2020|\n",
            "|  16|20.02.2020|\n",
            "|  17|10.05.2020|\n",
            "|  18|25.05.2020|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXm6qNRrzUb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "64b13b0a-fae4-4baa-d541-01e020011ad3"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as age,_c23 as date\n",
        "          FROM age_death_table\n",
        "          WHERE _c23 is not null AND _c1 is not null\n",
        "          ORDER BY _c1 DESC\n",
        "          \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|  age|                date|\n",
            "+-----+--------------------+\n",
            "|  age|date_death_or_dis...|\n",
            "|   99|          03.05.2020|\n",
            "|   95|          03.03.2020|\n",
            "|   95|          03.03.2020|\n",
            "|   94|          14.02.2020|\n",
            "|   92|          03.03.2020|\n",
            "|   91|          18.04.2020|\n",
            "|90-99|          03.03.2020|\n",
            "|90-99|          08.03.2020|\n",
            "|   90|          27.04.2020|\n",
            "|    9|          18.02.2020|\n",
            "|   89|          20.03.2020|\n",
            "|   89|          28.03.2020|\n",
            "|   89|          19.01.2020|\n",
            "|   89|          18.01.2020|\n",
            "|   89|          22.05.2020|\n",
            "|   88|          12.03.2020|\n",
            "|   88|          01.03.2020|\n",
            "|   87|          23.01.2020|\n",
            "|   87|          14.03.2020|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdLDILAvzbO4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "3b8a03cc-0dc5-4164-a4ab-07668012b1d6"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as age,_c22 as outcome,_c23 as date\n",
        "          FROM age_death_table\n",
        "          WHERE _c23 is not null AND _c1 is not null\n",
        "          ORDER BY _c1 DESC\n",
        "          \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+--------------------+\n",
            "|  age|   outcome|                date|\n",
            "+-----+----------+--------------------+\n",
            "|  age|   outcome|date_death_or_dis...|\n",
            "|   99|      died|          03.05.2020|\n",
            "|   95|     death|          03.03.2020|\n",
            "|   95|     death|          03.03.2020|\n",
            "|   94|discharged|          14.02.2020|\n",
            "|   92|      died|          03.03.2020|\n",
            "|   91|      died|          18.04.2020|\n",
            "|90-99|      died|          03.03.2020|\n",
            "|90-99|      died|          08.03.2020|\n",
            "|   90|      died|          27.04.2020|\n",
            "|    9|discharged|          18.02.2020|\n",
            "|   89|      died|          20.03.2020|\n",
            "|   89|      died|          28.03.2020|\n",
            "|   89|      died|          19.01.2020|\n",
            "|   89|      died|          18.01.2020|\n",
            "|   89|      died|          22.05.2020|\n",
            "|   88|      died|          12.03.2020|\n",
            "|   88|     death|          01.03.2020|\n",
            "|   87|      died|          23.01.2020|\n",
            "|   87|      died|          14.03.2020|\n",
            "+-----+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey2CQMe4zp-0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "bb50058c-8c09-431b-e750-86d9353efcc6"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as age,_c22 as outcome,_c23 as date\n",
        "          FROM age_death_table\n",
        "          WHERE _c23 is not null AND _c1 is not null\n",
        "          ORDER BY _c1 ASC\n",
        "          \n",
        "          '''\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------+----------+\n",
            "| age|   outcome|      date|\n",
            "+----+----------+----------+\n",
            "|   0|      died|16.05.2020|\n",
            "|   0|      died|12.05.2020|\n",
            "|   0|      died|09.05.2020|\n",
            "|0.25| discharge|20.02.2020|\n",
            "|0.25| discharge|20.02.2020|\n",
            "| 0.5|discharged|22.02.2020|\n",
            "|   1| discharge|18.02.2020|\n",
            "|   1| discharge|18.02.2020|\n",
            "|   1|      died|21.05.2020|\n",
            "|   1|discharged|17.02.2020|\n",
            "|   1|      died|12.05.2020|\n",
            "|   1|      died|08.05.2020|\n",
            "|  10|      died|14.05.2020|\n",
            "|  12| discharge|07.03.2020|\n",
            "|  12| discharge|07.03.2020|\n",
            "|  13| recovered|16.03.2020|\n",
            "|  13|      null|10.03.2020|\n",
            "|  16| discharge|20.02.2020|\n",
            "|  17|      died|10.05.2020|\n",
            "|  18|      died|25.05.2020|\n",
            "+----+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPYIOwsb81HW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a478a354-5f5b-4c01-bc76-4b7283730f0d"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT mean(_c1) as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c22 IS NOT NULL\n",
        "          ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#1082, _c22#36 AS outcome#1083]\n+- Filter (isnotnull(_c1#15) AND isnotnull(_c22#36))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10$adapted(CheckAnalysis.scala:255)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:282)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-766959f7a332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_c22\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''').show()\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#1082, _c22#36 AS outcome#1083]\n+- Filter (isnotnull(_c1#15) AND isnotnull(_c22#36))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwaxxAYL9IDa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "612dd697-4d96-4e87-b4d1-c026a0bf3bb7"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT _c1 as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c22 = \"death\"\n",
        "          ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+\n",
            "|mean_age|outcome|\n",
            "+--------+-------+\n",
            "|      78|  death|\n",
            "|      68|  death|\n",
            "|      88|  death|\n",
            "|      95|  death|\n",
            "|   80-89|  death|\n",
            "|      60|  death|\n",
            "|      79|  death|\n",
            "|      52|  death|\n",
            "|      66|  death|\n",
            "|      82|  death|\n",
            "|      73|  death|\n",
            "|   80-89|  death|\n",
            "|   60-69|  death|\n",
            "|      77|  death|\n",
            "|   80-89|  death|\n",
            "|   60-69|  death|\n",
            "|      95|  death|\n",
            "|      79|  death|\n",
            "|      82|  death|\n",
            "+--------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rzQ6pAt9PJo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2a0d627-0aad-4210-cf63-66f9eab87a56"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT mean(_c1) as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c1 = 40\n",
        "          ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#1106, _c22#36 AS outcome#1107]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10$adapted(CheckAnalysis.scala:255)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:282)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-e82079a63286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''').show()\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#1106, _c22#36 AS outcome#1107]\n+- Filter (isnotnull(_c1#15) AND (cast(_c1#15 as int) = 40))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpP077Wj9ZeG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "201e06ff-b968-4e2b-88ba-76086e4b2abe"
      },
      "source": [
        "spark.sql('''\n",
        "          SELECT mean(_c1) as mean_age, _c22 as outcome\n",
        "          FROM age_death_table\n",
        "          WHERE _c1 IS NOT NULL and _c22 = \"death\"\n",
        "          ''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#1109, _c22#36 AS outcome#1110]\n+- Filter (isnotnull(_c1#15) AND (_c22#36 = death))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$10$adapted(CheckAnalysis.scala:255)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:282)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:154)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:148)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:69)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-8809cf42fc57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mFROM\u001b[0m \u001b[0mage_death_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mWHERE\u001b[0m \u001b[0m_c1\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_c22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"death\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           ''').show()\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \"\"\"\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: grouping expressions sequence is empty, and 'age_death_table.`_c22`' is not an aggregate function. Wrap '(avg(CAST(age_death_table.`_c1` AS DOUBLE)) AS `mean_age`)' in windowing function(s) or wrap 'age_death_table.`_c22`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [avg(cast(_c1#15 as double)) AS mean_age#1109, _c22#36 AS outcome#1110]\n+- Filter (isnotnull(_c1#15) AND (_c22#36 = death))\n   +- SubqueryAlias `age_death_table`\n      +- RelationV2[_c0#14, _c1#15, _c2#16, _c3#17, _c4#18, _c5#19, _c6#20, _c7#21, _c8#22, _c9#23, _c10#24, _c11#25, _c12#26, _c13#27, _c14#28, _c15#29, _c16#30, _c17#31, _c18#32, _c19#33, _c20#34, _c21#35, _c22#36, _c23#37, ... 9 more fields] csv file:/content/data/COVID19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK0Bp42OCAyc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "de9f68ee-3a83-4393-be17-25bf896a6197"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 9))\n",
        "ax  = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.bar(x=[1, 2, 3], \n",
        "       height=[0.15, 0.5, 0.35],\n",
        "       color=['red', 'green', 'blue'],\n",
        "       tick_label=[\"Death\", \"Recover\", \"discharge\"],\n",
        "       width=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAIICAYAAABkYYgLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW70lEQVR4nO3df/Bld13f8dfbjVE0GXBksW1+sJFufyyIQJZoabWIyCR2msCATjJ1KpUai6ZCQadxsFFTpkVQOjrGSooM1koTQnW61NXY0h9KBzALJECIabcRzGaYyYpAiyAYfPePexa/LJvsTXK/+WbzfjxmGO4595NzP9+Bz/c+c+6531PdHQAAmOZLdnoCAACwE4QwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADDSaTv1wo997GN7z549O/XyAAAM8e53v/sPu3v38ft3LIT37NmTQ4cO7dTLAwAwRFV9+ET7XRoBAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASGuFcFVdWFW3V9XhqrryBM+/sKqOVtXNy3/+4eanCgAAm3PayQZU1a4k1yT5tiRHktxUVQe6+4PHDb2+u6/YhjkCAMDGrXNG+IIkh7v7ju7+bJLrklyyvdMCAIDttU4In5Xkzi3bR5Z9x3t+Vb2vqt5SVedsZHYAALBNTnppxJremuTfd/dnqur7kvxSkmcdP6iqLk9yeZKce+65G3ppYKr6idrpKfAQ6B/rnZ4C8Ai1zhnhu5JsPcN79rLv87r7o939mWXz9UnOP9GBuvva7t7f3ft37979QOYLAAAbsU4I35Rkb1WdV1WnJ7k0yYGtA6rqL27ZvDjJbZubIgAAbN5JL43o7nuq6ookNybZleQN3X1rVV2d5FB3H0jyg1V1cZJ7kvxRkhdu45wBAOBBW+sa4e4+mOTgcfuu2vL4R5L8yGanBgAA28ed5QAAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICR1grhqrqwqm6vqsNVdeV9jHt+VXVV7d/cFAEAYPNOGsJVtSvJNUkuSrIvyWVVte8E485M8pIk79r0JAEAYNPWOSN8QZLD3X1Hd382yXVJLjnBuH+e5CeT/MkG5wcAANtinRA+K8mdW7aPLPs+r6qeluSc7v71+zpQVV1eVYeq6tDRo0fv92QBAGBTHvSX5arqS5K8NsnLTza2u6/t7v3dvX/37t0P9qUBAOABWyeE70pyzpbts5d9x5yZ5ElJ/ntVfSjJNyY54AtzAAA8nK0Twjcl2VtV51XV6UkuTXLg2JPd/Ynufmx37+nuPUnemeTi7j60LTMGAIANOGkId/c9Sa5IcmOS25K8ubtvraqrq+ri7Z4gAABsh9PWGdTdB5McPG7fVfcy9pkPfloAALC93FkOAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkU7b6QkAALNV7fQMeCh07/QMvpgzwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGGmtEK6qC6vq9qo6XFVXnuD5f1RV76+qm6vq7VW1b/NTBQCAzTlpCFfVriTXJLkoyb4kl50gdN/U3V/X3U9J8uokr934TAEAYIPWOSN8QZLD3X1Hd382yXVJLtk6oLv/75bNr0zSm5siAABs3mlrjDkryZ1bto8k+YbjB1XVDyR5WZLTkzzrRAeqqsuTXJ4k55577v2dKwAAbMzGvizX3dd09xOS/NMkP3ovY67t7v3dvX/37t2bemkAALjf1gnhu5Kcs2X77GXfvbkuyXMfzKQAAGC7rRPCNyXZW1XnVdXpSS5NcmDrgKrau2Xz7yT535ubIgAAbN5JrxHu7nuq6ookNybZleQN3X1rVV2d5FB3H0hyRVU9O8mfJvlYku/ezkkDAMCDtc6X5dLdB5McPG7fVVsev2TD8wIAgG3lznIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYKS1QriqLqyq26vqcFVdeYLnX1ZVH6yq91XV26rq8ZufKgAAbM5JQ7iqdiW5JslFSfYluayq9h037L1J9nf3k5O8JcmrNz1RAADYpHXOCF+Q5HB339Hdn01yXZJLtg7o7v/W3Z9aNt+Z5OzNThMAADZrnRA+K8mdW7aPLPvuzYuS/MaJnqiqy6vqUFUdOnr06PqzBACADdvol+Wq6ruS7E/ymhM9393Xdvf+7t6/e/fuTb40AADcL6etMeauJOds2T572fcFqurZSV6R5G9392c2Mz0AANge65wRvinJ3qo6r6pOT3JpkgNbB1TVU5O8LsnF3X335qcJAACbddIQ7u57klyR5MYktyV5c3ffWlVXV9XFy7DXJDkjyQ1VdXNVHbiXwwEAwMPCOpdGpLsPJjl43L6rtjx+9obnBQAA28qd5QAAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABjptJ2ewI6o2ukZsN26d3oGAMDDnDPCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYaa0QrqoLq+r2qjpcVVee4Plvrqr3VNU9VfWCzU8TAAA266QhXFW7klyT5KIk+5JcVlX7jhv2B0lemORNm54gAABsh9PWGHNBksPdfUeSVNV1SS5J8sFjA7r7Q8tzf7YNcwQAgI1b59KIs5LcuWX7yLIPAABOWQ/pl+Wq6vKqOlRVh44ePfpQvjQAAHyBdUL4riTnbNk+e9l3v3X3td29v7v37969+4EcAgAANmKdEL4pyd6qOq+qTk9yaZID2zstAADYXicN4e6+J8kVSW5McluSN3f3rVV1dVVdnCRV9fSqOpLkO5K8rqpu3c5JAwDAg7XOX41Idx9McvC4fVdteXxTVpdMAADAKcGd5QAAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASEIYAICRhDAAACMJYQAARhLCAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJGEMAAAIwlhAABGEsIAAIwkhAEAGEkIAwAwkhAGAGAkIQwAwEhCGACAkYQwAAAjCWEAAEYSwgAAjCSEAQAYSQgDADCSEAYAYCQhDADASGuFcFVdWFW3V9XhqrryBM9/WVVdvzz/rqras+mJAgDAJp00hKtqV5JrklyUZF+Sy6pq33HDXpTkY939l5P8qyQ/uemJAgDAJq1zRviCJIe7+47u/myS65JcctyYS5L80vL4LUm+tapqc9MEAIDNWieEz0py55btI8u+E47p7nuSfCLJV29iggAAsB1OeyhfrKouT3L5svnJqrr9oXz9wR6b5A93ehIPKR9I8Mg1bj3Xj1vPPCLNW8s7u5Qff6Kd64TwXUnO2bJ99rLvRGOOVNVpSR6d5KPHH6i7r01y7TqzZXOq6lB379/peQAPnvUMjwzW8sPDOpdG3JRkb1WdV1WnJ7k0yYHjxhxI8t3L4xck+a/d3ZubJgAAbNZJzwh39z1VdUWSG5PsSvKG7r61qq5Ocqi7DyT5xSS/XFWHk/xRVrEMAAAPW+XE7SNfVV2+XJYCnOKsZ3hksJYfHoQwAAAjucUyAAAjCeFTSFV9rqpurqpbq+qWqnp5VT2g/w2r6jFV9f1btp9ZVf9pc7MFjtmydj9QVW+tqsfs9JyA+6eqfryqfqiqrq6qZ9/Pf3ZPVX1gu+bGAyeETy2f7u6ndPcTk3xbVre9/rEHeKzHJPn+k44CNuHY2n1SVl8o/oGdnlCS1Ir3Abgfuvuq7v4vD+VrLn+alm3gF+ApqrvvzurmJFcsb2a7quo1VXVTVb2vqr4vSarqjKp6W1W9p6reX1XHbo/9qiRPWM5SvWbZd0ZVvaWqfq+qfsVtsmFbvCPL3Tmr6glV9ZtV9e6q+p2q+mvL/q+pql9bPvm5paqesex/2XJW+QNV9dJl36uq6vNhfeys1fL4h7f8TviJZd+eqrq9qv5tkg/kC/9OPLBFVb2iqv5XVb09yV9d9r2xql6wPH5VVX1wWWM/tew74fpNsquq/s3yqe5vVdWjlvHfu6zTW6rqP1TVV2x5nV+oqnclefXy++Kdy3v5K6vqk1vm+UVrnfUI4VNYd9+R1Z+0e1ySFyX5RHc/PcnTk3xvVZ2X5E+SPK+7n5bkW5L89BK4Vyb5P8tZqh9eDvnUJC9Nsi/J1yb5mw/pDwSPcFW1K8m35s//Fvu1Sf5xd5+f5IeS/Pyy/2eT/I/u/vokT0tya1Wdn+QfJPmGJN+Y1Rp/apLrk3znlpf5ziTXV9VzkuxNckGSpyQ5v6q+eRmzN8nPd/cTu/vD2/PTwqltWXOXZrV+vj2r99atz391kucleWJ3PznJK5envmj9Lvv3Jrlm+VT340mev+z/1e5++jL+tqzez485O8kzuvtlSX4myc9099clObJlHve11jkJp9ofOZ6T5MnH/i01q7v77c1qsfyLZVH8WVZnor7mXo7xu919JEmq6uYke5K8fTsnDUM8allTZ2X1Rvefq+qMJM9IcsOWD1++bPnvZyX5+0nS3Z9L8omq+ltJfq27/zhJqupXk3xTd/9sVT2uqv5Skt1JPtbdd1bVS7L6vfDe5ZhnZPU74Q+SfLi737m9PzKc8r4pqzX3qSSpquNvJvaJrE42/eLyHZtj37M50fr9qiS/3903L2PendV7bJI8qapemdUli2dkdd+GY25YjpEkfyPJc5fHb0ryU8vj5+TEa/23H8DPPI4QPoVV1dcm+VySu5NUVmeWbjxuzAuzenM8v7v/tKo+lOTL7+WQn9ny+HPx/w/YlE9391OWjzxvzOoa4Tcm+Xh3P2UDx78hq7t6/oWszhAnq98J/7K7X7d1YFXtSfLHG3hNGG254dgFWX3K84IkV2QVwffm+PfYRy2P35jkud19y/Ke/cwt49ZZqydc66zHpRGnqKraneQXkvzccjvrG5O8uKq+dHn+r1TVV2Z1ZvjuJYK/Jcnjl0P8vyRn7sDUYazlzNIPJnl5kk8l+f2q+o7k819c+/pl6NuSvHjZv6uqHp3kd5I8t6q+Ylnbz1v2Jav4vTSrN+Mbln03Jvme5cxzquqsqnrcdv+M8Ajy21mtuUdV1ZlJ/u7WJ5e19ejuPpjknyS5r/V7X85M8pHl/fvv3ce4d+bPL6fYegdfa/1BcMbv1HLs49UvTXJPkl9O8trluddn9THLe5ZrgI9m9RHKryR5a1W9P8mhJL+XJN390ar6n7X6cy6/keTXH8ofBKbq7vdW1fuSXJbVm96/rqofzWpdX5fkliQvSXJtVb0oqzNHL+7ud1TVG5P87nKo13f3e5dj3rq8Ud/V3R9Z9v1WVf31JO9YLr34ZJLvWo4HnER3v6eqrs9qTd6d5KbjhpyZ5D9W1ZdndVb2Zcv+L1q/ST5yHy/1z5K8K6v37Xfl3k9SvTTJv6uqVyT5zawuzbivtX73+j/tXO4sBwDwMLdcWvXp7u6qujTJZd19ycn+Oe6bM8IAAA9/5yf5ueVT348n+Z4dns8jgjPCAACM5MtyAACMJIQBABhJCAMAMJIQBgBgJCEMAMBIQhgAgJH+P3bC1BY2t4jdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}